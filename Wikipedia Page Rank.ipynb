{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"49b1ee56-9657-4283-87fa-491295fd3ab2","showTitle":false,"title":""}},"source":["# Wikipedia Page Rank\n","\n","PageRank is an algorithm that is used to determine the importance of a webpage or a set of webpages. It was developed by Larry Page and Sergey Brin, the founders of Google, as a way to rank webpages in the search engine's results. The basic idea behind PageRank is that a webpage is important if it is linked to by many other important webpages.\n","\n","The algorithm works by treating each webpage as a node in a directed graph, with edges representing links between webpages. The importance of a webpage (its PageRank) is then computed based on the importance of the webpages that link to it. The algorithm uses a recursive formula to iteratively calculate the PageRank of each webpage, until a stable rank is reached.\n","\n","\n","\n","The provided code is an implementation of the PageRank algorithm which is a method used to determine the importance or relevance of a page in a website based on the number and quality of links that point to it. The algorithm is typically used to rank web pages in search engine results."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6f1ca151-3c59-47a8-b0ad-433c8fd3ddce","showTitle":false,"title":""}},"source":["## Part 0: Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"000ae955-cd3b-4a52-9e0c-978241c38d5c","showTitle":false,"title":""}},"outputs":[],"source":["import pandas as pd\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"10865609-2787-4474-900b-54e4dd830765","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.types import *\n","from pyspark.sql.types import ArrayType, StringType,LongType\n","from pyspark.sql.functions import size, explode, collect_list \n","from pyspark.sql.functions import col\n","from pyspark.sql.functions import *"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7ae6143a-d173-4019-861b-e9955817c173","showTitle":false,"title":""}},"outputs":[],"source":["spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0acb0187-39c5-4fad-8e2e-e3e57352967c","showTitle":false,"title":""}},"source":["## Part 1: Get the data \n","\n","- read a parquet file and create a DataFrame\n","- count the total number of rows in the DataFrame\n","- take a random sample of the DataFrame with a fraction of 0.001 and random seed 0, and cache it in memory"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"29455663-e769-4c76-a3b0-370d20b553a8","showTitle":false,"title":""}},"outputs":[],"source":["wikipediaDF=spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e16e48be-33f7-47c7-8265-a94866829c01","showTitle":false,"title":""}},"outputs":[],"source":["# Count the total rows of the DF\n","N=wikipediaDF.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8f1bd9c8-7f2f-4780-b338-8e55fbd947a1","showTitle":false,"title":""}},"outputs":[],"source":["PartialWikipediaDF=wikipediaDF.sample(fraction=0.0001,seed=0).cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0a14781c-d035-43fc-bf5b-b5a21992fc51","showTitle":false,"title":""}},"outputs":[],"source":["\n","#PartialWikipediaDF.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"01e0bba7-8708-4e6e-b3b9-b4e00e5adf12","showTitle":false,"title":""}},"outputs":[],"source":["#display(PartialWikipediaDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"45dae667-680f-4454-ae07-33f503f14f8d","showTitle":false,"title":""}},"source":["## Part 2: Parse the link in the data frame\n","Using a udf(user defined function) parse the text field from each record"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"59c29a9a-f2f1-4773-bbdc-ce0b625ab87e","showTitle":false,"title":""}},"source":["### (2a) define the function and the udf"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"77276afc-1f7c-4a23-860c-c923682a882e","showTitle":false,"title":""}},"outputs":[],"source":["# function which parses the text field from each record, and extracts the outgoing links.\n","def parse_links(document_body):\n","  # Find all strings that match the pattern [[...]]\n","  data = re.findall(r'\\[\\[(.+?)\\]\\]',document_body)\n","  \n","  # If there are any matches\n","  if (len(data) > 0):\n","    # convert all the matches to lowercase\n","    links = [s.lower() for s in data]\n","  else:\n","    # if there are no matches, return an empty list\n","    links = []\n","  return links\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"91652eea-ca38-4c8d-9291-7ad0f60fd66f","showTitle":false,"title":""}},"outputs":[],"source":["# define the udf for parsing a link\n","parse_links_udf = udf(parse_links,ArrayType(StringType()))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a9c9ce40-6272-43bc-b370-eb7f6ee6ecce","showTitle":false,"title":""}},"outputs":[],"source":["tolower_udf= udf(lambda x: x.lower())"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b48ecaba-f52d-4b34-81db-d1aaccfc2bf8","showTitle":false,"title":""}},"source":["## (2b) create a title dataframe\n","title_idDF is created by selecting the lowercase title and id fields from wikipediaDF. And it's been converted to pandas dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5c68b849-db6c-4747-9e48-c3995988ce80","showTitle":false,"title":""}},"outputs":[],"source":["title_idDF=wikipediaDF.select(lower(\"title\").alias(\"title\"),\"id\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"50b3e884-df1f-4e8d-ad61-9dbea9f6eea0","showTitle":false,"title":""}},"outputs":[],"source":["#convert the spark df to pandas\n","title_idPDF=title_idDF.toPandas()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"fc4ac4bd-b3fc-4347-b68b-30448f26378b","showTitle":false,"title":""}},"source":["## (2c) create a dataframe with the parsed link\n","TempForwardDF is created by selecting the title, id, and links (using parse_links_udf function on text field) fields from PartialWikipediaDF."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6df730cf-18f6-419b-be9e-0e049841c7b5","showTitle":false,"title":""}},"outputs":[],"source":["TempForwardDF=PartialWikipediaDF.select(\"title\",\"id\",parse_links_udf(\"text\").alias(\"links\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f5ec5da9-69fd-45ec-b476-7cc02030ff1b","showTitle":false,"title":""}},"outputs":[],"source":["#display(TempForwardDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"17bc42db-80ef-470d-9dd0-b953012cf43d","showTitle":false,"title":""}},"source":["## Part 3: create a Forward data frame \n","Create a forward df with the forwarding link of each node"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b67abdaf-0afc-4908-a5cb-82a089d99a43","showTitle":false,"title":""}},"source":["## (3a)  convert titles of the link into their ids\n","Function called titles2id is defined, which takes in a list of links and a pandas dataframe with titles and ids, and returns a list of ids corresponding to the links that are present in the pandas dataframe. This function is then wrapped in a udf called titles2id_UDF, which is used to create a new DataFrame called ForwardDF."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9fad811e-b47e-4c4f-bbf5-f4528f372393","showTitle":false,"title":""}},"outputs":[],"source":["# function that search and replace the links with the ids of the corresponding documents, to handle just numbers\n","def titles2id(links,titleidPDF):\n","  # df with titles and ids\n","  data_titles=titleidPDF\n","  # if row has outgoing links\n","  if (len(links)>0):\n","    # check if a title is in the links and append his id to a list \n","    ids=data_titles[data_titles.title.isin(links)].id.to_list()\n","  else:\n","    ids=[]\n","  return list(set(ids))"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a616e871-43fe-4a51-a0ad-f56b52855e4b","showTitle":false,"title":""}},"outputs":[],"source":["titles2id_UDF=udf(lambda x: titles2id(x,title_idPDF),ArrayType(LongType(),False))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b72b0e7d-2b8a-4f27-8456-d96db04aa691","showTitle":false,"title":""}},"source":["## (3b) Create the forward df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5a7de5e1-c880-47b3-8b33-369a0d436600","showTitle":false,"title":""}},"outputs":[],"source":["ForwardDF=TempForwardDF.select(\"id\",titles2id_UDF(\"links\").alias(\"links\"),size(\"links\").alias(\"counter\")).cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"da2335bf-8073-470d-98d4-45c34fb43b80","showTitle":false,"title":""}},"outputs":[],"source":["#display(ForwardDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"03ea5461-5f25-475d-b065-45c6bb226f01","showTitle":false,"title":""}},"source":["## Part 4: Create the reverse dataframe, and the page rank dataframe \n","Create the reverse data frame that will be used lately to compute the page rank algorithm.\n","\n","This dataframe will have an id(the id of the node), the links that are pointing to the node, and the counters of the links that point to the node."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0caad556-e024-4d7f-81f4-89669eba2516","showTitle":false,"title":""}},"source":["## (4a) create the df for the outgoing links\n","For each link select how many links are outgoing from him"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d9f97107-c3c0-4cbd-a3d5-b5f5249f9942","showTitle":false,"title":""}},"outputs":[],"source":["OutgoingsLinksCountersDF=ForwardDF.select(\"id\",\"counter\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3e55fd7d-2303-419f-abe2-337b72f93469","showTitle":false,"title":""}},"outputs":[],"source":["#display(OutgoingsLinksCountersDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c62eb634-597c-4390-8676-f7003e587af4","showTitle":false,"title":""}},"source":["## (4b) Create a temporal reverse link dataframe\n","\n","The select() function is used to select specific columns from the \"ForwardDF\" DataFrame, and in this case, it's selecting the \"id\" column and the \"counter\" column. \n","\n","Then it's using the explode() function on the \"links\" column and creating a new column called \"t_link\". The explode() function will create a new row for each element in the \"links\" column, so if the \"links\" column contains a list of n elements, the resulting DataFrame will have n rows, each with a copy of the \"id\" and \"counter\" columns and a single element from the \"links\" column in the \"t_link\" column."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"59dca80a-2a3a-4118-b079-075aade7764b","showTitle":false,"title":""}},"outputs":[],"source":["TemporalReverseLinks=ForwardDF.select(\"id\",explode(\"links\").alias(\"t_link\"),\"counter\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"af5418c5-1a1a-40d1-8e1f-9c9694354b8d","showTitle":false,"title":""}},"outputs":[],"source":["#display(TemporalReverseLinks)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3f7a5bbb-2423-4205-ae9a-f2a820e43eab","showTitle":false,"title":""}},"source":["## (4c) Create the reverse df\n","reverseDF is grouping the TemporalReverseLinks DataFrame by the \"t_link\" column, then using the groupBy method to perform two aggregation operations:\n","\n","- Using collect_list() function to create a new column \"Links\" which contains a list of all the values of \"id\" column in the group.\n","\n","- Using collect_list() function to create a new column \"counters\" which contains a list of all the values of \"counter\" column in the group.\n","  then it renaming the \"t_link\" column to \"id\" using withColumnRenamed() function.\n","  And it's caching the ReverseDF Dataframe for faster access in future operations.\n","  It's important to note that without the context of the rest of the program and the structure of the TemporalReverseLinks DataFrame, it's hard to say exactly what this line of code does and what the final DataFrame will look like."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7ddc69e7-0948-4a6e-a832-ea52bb8212bb","showTitle":false,"title":""}},"outputs":[],"source":["ReverseDF=TemporalReverseLinks.groupBy(\"t_link\").agg(collect_list (\"id\").alias(\"Links\"),collect_list(\"counter\").alias(\"counters\")).withColumnRenamed(\"t_link\",\"id\").cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"289f1f6f-925f-45e2-80c4-cd0852e257a7","showTitle":false,"title":""}},"outputs":[],"source":["#display(ReverseDF)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6ff0212c-696a-45e2-9b18-e8a2394d6871","showTitle":false,"title":""}},"outputs":[],"source":["ReverseDF.printSchema()\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d25aaa89-da72-4a2e-aa96-d2533bf94513","showTitle":false,"title":""}},"source":["## (4d) Create the page rank df\n","Create a DataFrame with the id and Page Rank initialized with 0.85/N and convert it to pandas dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d66dbf71-2d5b-43b0-91a0-8ec33278f99b","showTitle":false,"title":""}},"outputs":[],"source":["# add column PR\n","pageRankDF=ReverseDF.select(\"id\").withColumn(\"PR\",lit(0.85/N))\n","pageRankPDF=pageRankDF.toPandas()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6c3d079d-6d40-4656-b71d-155750e069a2","showTitle":false,"title":""}},"outputs":[],"source":["pageRankDF.printSchema()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"116572ae-1d3b-4128-9972-1c31b50ec35b","showTitle":false,"title":""}},"outputs":[],"source":["#display(pageRankPDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0f9596bc-bf2b-4980-a161-eb20216c96a5","showTitle":false,"title":""}},"source":["## Part 5: Compute the algorithm\n","\n","Finally, the PageRank algorithm is implemented by iterating over the ReverseDF, updating the Page Rank values of each page based on the links and counters, and checking for convergence of the algorithm."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6f481acf-ce8d-4421-ae34-1c96bf4558d2","showTitle":false,"title":""}},"source":["## (5a) Define the function for calculating the new page rank\n","Define a function that calculates the new Page Rank for a given document"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"9480bf63-2c3a-4633-9875-c89d1fa92518","showTitle":false,"title":""}},"outputs":[],"source":["\n","def new_pagerank(links, current_pr, counters):\n","  n_pr = 0;\n","  # for each incoming link to the page calculate the pagerank\n","  # zip the links and counters to get them both in the loop\n","  for l, c in zip(links, counters):\n","    \n","    \n","    # get current_pr of the link l\n","    try:\n","      current_link_pr=current_pr[current_pr['id']==l].PR.item()\n","      \n","    # this except is used in case that some links doesn't have a page rank\n","    # this can occurr when using a sample data frame\n","    except:\n","      current_link_pr=0.85/N\n","        \n","    # update the new Page Rank by adding the contribution of this link\n","    n_pr += current_link_pr/c\n","  new_pr = 0.85/N+0.15*n_pr\n","  return new_pr\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"33eeabe2-beab-46d4-ad03-cf5b193a9c70","showTitle":false,"title":""}},"source":["## (5b) Define the function for checking if  is converged\n","Define a function that checks if the Page Rank has converged,checking if the relative error between two values (current and previous) is less than or equal to a certain threshold (0.00001)."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cdfa5b97-8381-4d52-9e05-5aef2090b251","showTitle":false,"title":""}},"outputs":[],"source":["def converged(current,previous):\n","    relative_error = abs(current - previous) / abs(previous)\n","    return relative_error <= 0.00001\n","    \n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ff41937b-ab61-4257-9abd-03795c559b93","showTitle":false,"title":""}},"source":["## (5c) Compute the algorithm \n"," It uses a while loop to iterate until the Page Rank has converged or the maximum number of iterations (20) is reached. \n"," \n"," The loop starts by calculating the new Page Rank of each document using the new_pagerank function. \n"," \n"," This function takes in the links and counters of a document and the current Page Rank DataFrame, then calculates the new Page Rank by adding the contribution of each incoming link.\n"," \n"," After that, it checks for convergence by comparing the new and previous Page Rank DataFrames. If all the documents have converged, the loop is stopped. \n"," \n"," Finally, the final Page Rank DataFrame is displayed."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b205b183-6564-4b2d-996c-bcc32e86497d","showTitle":false,"title":""}},"outputs":[],"source":["# create a UDF that applies the new_pagerank function to the DataFrame\n","new_pagerank_udf = udf(lambda x,y: new_pagerank(x,pageRankPDF,y), DoubleType())  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"3c5835e2-fe31-423a-b161-95998556d72d","showTitle":false,"title":""}},"outputs":[],"source":["# create a DataFrame to store the previous Page Rank(used for checking if is converged or not)\n","PreviousPageRankDF=pageRankDF\n","count=0;\n","flag=True\n","# iterate until the Page Rank has converged or the maximum number of iterations is reached\n","while ((flag==True) & (count<20)):\n","    # calculate the new Page Rank\n","    NewPageRankDF=ReverseDF.select(\n","        ReverseDF[\"id\"],\n","        new_pagerank_udf(ReverseDF[\"links\"],ReverseDF[\"counters\"]).alias(\"PR\"))\n","    pageRankPDF=NewPageRankDF.toPandas()\n","    \n","    # join the new and previous Page Rank DataFrames to check for convergence\n","    checkConvergenceDF=NewPageRankDF.withColumnRenamed(\"PR\",\"New_PR\").join(PreviousPageRankDF,NewPageRankDF[\"id\"] == PreviousPageRankDF[\"id\"])\n","    \n","    # check if the Page Rank has converged\n","    checkConvergenceDF=checkConvergenceDF.withColumn(\"is_converged\",converged(checkConvergenceDF[\"New_PR\"],checkConvergenceDF[\"PR\"]))\n","    # exit condition if all the rows are converged\n","    if checkConvergenceDF.filter(col(\"is_converged\")).count() == checkConvergenceDF.count():\n","        flag=False\n","\n","    # update the udf with the new pagerank pdf\n","    new_pagerank_udf = udf(lambda x,y: new_pagerank(x,pageRankPDF,y), DoubleType())\n","    display(pageRankPDF)\n","    \n","    PreviousPageRankDF=NewPageRankDF\n","    count=count+1\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cb4d4d5f-33c2-4abb-af46-2d1596aa48cd","showTitle":false,"title":""}},"outputs":[],"source":["# display the final Page Rank DataFrame\n","display(pageRankPDF)"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6e0f1f2b-758f-4624-aeeb-c3122f0f03cc","showTitle":false,"title":""}},"source":["## Analysis"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c0268b69-b2c8-4e5e-9a3e-f7289c531dc9","showTitle":false,"title":""}},"source":["### Sample dataframe problem\n","Using a Sample dataframe the PageRank algorithm is converging after just a few iterations because some of the nodes that are pointed to by nodes in the sample data frame are not present in the sample data frame.\n","\n","These missing nodes would not have a PageRank value assigned to them, which could cause the algorithm to converge faster.\n","\n","This is because the PageRank algorithm relies on the links between pages to propagate the rank scores, and if some of these links are missing, the algorithm can't continue to propagate the rank scores and thus converge faster.\n","\n","A solution is use a try except construct to try to search the page rank of that link, and if not present assign a default value(0.85/N in this case).\n","Using this solution will give you good estimates of the page rank, but still not the most accurate because it will converge really fast(2/3 iteration for 0.0001 sample and 4/5 iteration for 0.001).\n","\n","Another alternative is to remove the missing nodes from the dataset and run the algorithm again to see if the result is more accurate.\n","\n","To get a more accurate estimate of the page rank values, you would need to run the algorithm on the full dataset, rather than just a sample. This will ensure that all the links between the pages are present and the algorithm can propagate the rank scores correctly. However, it may require significant computational resources depending on the size of the dataset."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"8da56ca2-1579-44fc-8cb4-782e44fc6818","showTitle":false,"title":""}},"source":["### Damping factor\n","The damping factor is a probability value that is used in the PageRank algorithm to handle the problem of web pages that form a directed cycle, leading to a situation where the algorithm would never converge.\n","\n","The damping factor is used to introduce a small probability that, at each step, the random surfer will \"teleport\" to a random webpage rather than following a link. This means that even if a webpage is in a cycle of pages that all link to each other, there is still a small chance that the random surfer will leave the cycle and explore other pages.\n","\n","The damping factor is typically set to a value between 0 and 1, with a common value being 0.85(as in this case). This means that at each step, there is a 15% chance that the random surfer will \"teleport\" to a random webpage.\n","\n","A damping factor of 0 means that the random surfer never teleports to other pages and will eventually get stuck in a loop. A damping factor of 1 means that the random surfer never follows links and only teleports to other pages. \n","\n","It is important to note that the damping factor value chosen will affect the result of the algorithm, and a different value might lead to different ranking of pages.\n","\n","It's also worth noting that the PageRank algorithm is susceptible to manipulation and can be gamed by spammers. Additionally, like all algorithm, the results of PageRank algorithm can be affected by the data bias and damping factor, because spammers often use link farms (a group of web pages that all link to each other) to increase the importance of their web pages, these pages will tend to have artificially high PageRank scores."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c1e0769e-9461-4491-9800-8785e9494364","showTitle":false,"title":""}},"source":["### Convergence\n","\n","Convergence in the context of the PageRank algorithm refers to the point at which the algorithm has iterated enough times that the page rank scores of the nodes in the network have stabilized and are not changing significantly with each iteration. \n","\n","This can be determined by comparing the page rank scores of the nodes between consecutive iterations and checking if they have reached a certain level of similarity. \n","\n","Once the algorithm has converged, the final page rank scores can be considered to be a good approximation of the true page rank of the nodes in the network.\n","\n","To check if is converged we use a relative error, a measure of how much the PageRank values have changed between iterations. It is calculated as the absolute difference between the current PageRank values and the previous PageRank values, divided by the absolute value of the previous PageRank values."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7e660380-0a21-47c0-979a-d74f7e9cb175","showTitle":false,"title":""}},"source":["### Conclusions\n","In conclusion, one of the main advantages of the PageRank algorithm is that it can be used to identify authoritative or important webpages in a set of web pages. \n","\n","This is because the algorithm takes into account not only the number of inbound links to a webpage but also the quality of those links, as the links from more authoritative web pages will pass on more importance to the linked web page.\n","\n","It's important to note that PageRank algorithm is a computational intensive algorithm, it's not suitable for large graphs and requires significant computational resources to run efficiently.\n","\n","In fact with limited resources, like we have using databricks community , the algorithm with the full wikipedia dataset is too slow to run."]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Wikipedia Page Rank","notebookOrigID":838356696754887,"widgets":{}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.11"}},"nbformat":4,"nbformat_minor":0}
